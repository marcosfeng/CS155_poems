{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/marcosgallo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/marcosgallo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/marcosgallo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/marcosgallo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/marcosgallo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/marcosgallo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import np_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "raw_text = load_doc('data/char_sequences.txt')\n",
    "lines = raw_text.split('\\n')\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "sequences = list()\n",
    "for line in lines:\n",
    "    # integer encode line\n",
    "    encoded_seq = [mapping[char] for char in line]\n",
    "    sequences.append(encoded_seq)\n",
    "    \n",
    "vocab_size = len(mapping)\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 75)                34200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 38)                2888      \n",
      "=================================================================\n",
      "Total params: 37,088\n",
      "Trainable params: 37,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "93787/93787 [==============================] - 85s 907us/step - loss: 2.2990 - accuracy: 0.3395\n",
      "Epoch 2/100\n",
      "93787/93787 [==============================] - 86s 922us/step - loss: 2.0545 - accuracy: 0.3953\n",
      "Epoch 3/100\n",
      "93787/93787 [==============================] - 87s 928us/step - loss: 1.9540 - accuracy: 0.4176\n",
      "Epoch 4/100\n",
      "93787/93787 [==============================] - 88s 941us/step - loss: 1.8888 - accuracy: 0.4360\n",
      "Epoch 5/100\n",
      "93787/93787 [==============================] - 94s 1ms/step - loss: 1.8385 - accuracy: 0.4493\n",
      "Epoch 6/100\n",
      "93787/93787 [==============================] - 87s 926us/step - loss: 1.7986 - accuracy: 0.4599\n",
      "Epoch 7/100\n",
      "93787/93787 [==============================] - 81s 868us/step - loss: 1.7659 - accuracy: 0.4686\n",
      "Epoch 8/100\n",
      "93787/93787 [==============================] - 84s 898us/step - loss: 1.7374 - accuracy: 0.4758\n",
      "Epoch 9/100\n",
      "93787/93787 [==============================] - 85s 905us/step - loss: 1.7136 - accuracy: 0.4818\n",
      "Epoch 10/100\n",
      "93787/93787 [==============================] - 86s 915us/step - loss: 1.6908 - accuracy: 0.4864\n",
      "Epoch 11/100\n",
      "93787/93787 [==============================] - 85s 908us/step - loss: 1.6713 - accuracy: 0.4920\n",
      "Epoch 12/100\n",
      "93787/93787 [==============================] - 85s 911us/step - loss: 1.6540 - accuracy: 0.4957\n",
      "Epoch 13/100\n",
      "93787/93787 [==============================] - 86s 915us/step - loss: 1.6368 - accuracy: 0.5009\n",
      "Epoch 14/100\n",
      "93787/93787 [==============================] - 86s 920us/step - loss: 1.6231 - accuracy: 0.5037\n",
      "Epoch 15/100\n",
      "93787/93787 [==============================] - 139s 1ms/step - loss: 1.6089 - accuracy: 0.5068\n",
      "Epoch 16/100\n",
      "93787/93787 [==============================] - 90s 954us/step - loss: 1.5972 - accuracy: 0.5111\n",
      "Epoch 17/100\n",
      "93787/93787 [==============================] - 92s 985us/step - loss: 1.5859 - accuracy: 0.5125\n",
      "Epoch 18/100\n",
      "93787/93787 [==============================] - 91s 974us/step - loss: 1.5756 - accuracy: 0.5150\n",
      "Epoch 19/100\n",
      "93787/93787 [==============================] - 90s 954us/step - loss: 1.5654 - accuracy: 0.5177\n",
      "Epoch 20/100\n",
      "93787/93787 [==============================] - 91s 970us/step - loss: 1.5557 - accuracy: 0.5206\n",
      "Epoch 21/100\n",
      "93787/93787 [==============================] - 88s 943us/step - loss: 1.5478 - accuracy: 0.5223\n",
      "Epoch 22/100\n",
      "93787/93787 [==============================] - 133s 1ms/step - loss: 1.5392 - accuracy: 0.5254\n",
      "Epoch 23/100\n",
      "93787/93787 [==============================] - 1180s 13ms/step - loss: 1.5314 - accuracy: 0.5263\n",
      "Epoch 24/100\n",
      "93787/93787 [==============================] - 93s 988us/step - loss: 1.5242 - accuracy: 0.5297\n",
      "Epoch 25/100\n",
      "93787/93787 [==============================] - 100s 1ms/step - loss: 1.5168 - accuracy: 0.5303\n",
      "Epoch 26/100\n",
      "93787/93787 [==============================] - 131s 1ms/step - loss: 1.5110 - accuracy: 0.5324\n",
      "Epoch 27/100\n",
      "93787/93787 [==============================] - 101s 1ms/step - loss: 1.5042 - accuracy: 0.5353\n",
      "Epoch 28/100\n",
      "93787/93787 [==============================] - 101s 1ms/step - loss: 1.4989 - accuracy: 0.5358\n",
      "Epoch 29/100\n",
      "93787/93787 [==============================] - 100s 1ms/step - loss: 1.4927 - accuracy: 0.5372\n",
      "Epoch 30/100\n",
      "93787/93787 [==============================] - 119s 1ms/step - loss: 1.4876 - accuracy: 0.5381\n",
      "Epoch 31/100\n",
      "93787/93787 [==============================] - 114s 1ms/step - loss: 1.4817 - accuracy: 0.5389\n",
      "Epoch 32/100\n",
      "93787/93787 [==============================] - 100s 1ms/step - loss: 1.4766 - accuracy: 0.5418\n",
      "Epoch 33/100\n",
      "93787/93787 [==============================] - 108s 1ms/step - loss: 1.4713 - accuracy: 0.5419\n",
      "Epoch 34/100\n",
      "93787/93787 [==============================] - 106s 1ms/step - loss: 1.4668 - accuracy: 0.5441\n",
      "Epoch 35/100\n",
      "93787/93787 [==============================] - 100s 1ms/step - loss: 1.4619 - accuracy: 0.5450\n",
      "Epoch 36/100\n",
      "93787/93787 [==============================] - 79s 846us/step - loss: 1.4578 - accuracy: 0.5469\n",
      "Epoch 37/100\n",
      "93787/93787 [==============================] - 90s 960us/step - loss: 1.4537 - accuracy: 0.5465\n",
      "Epoch 38/100\n",
      "93787/93787 [==============================] - 93s 990us/step - loss: 1.4554 - accuracy: 0.5486\n",
      "Epoch 39/100\n",
      "93787/93787 [==============================] - 97s 1ms/step - loss: 1.4430 - accuracy: 0.5508\n",
      "Epoch 40/100\n",
      "93787/93787 [==============================] - 120s 1ms/step - loss: 1.4417 - accuracy: 0.5507\n",
      "Epoch 41/100\n",
      "93787/93787 [==============================] - 98s 1ms/step - loss: 1.4382 - accuracy: 0.5511\n",
      "Epoch 42/100\n",
      "93787/93787 [==============================] - 98s 1ms/step - loss: 1.4350 - accuracy: 0.5526\n",
      "Epoch 43/100\n",
      "93787/93787 [==============================] - 100s 1ms/step - loss: 1.4309 - accuracy: 0.5535\n",
      "Epoch 44/100\n",
      "93787/93787 [==============================] - 95s 1ms/step - loss: 1.4272 - accuracy: 0.5551\n",
      "Epoch 45/100\n",
      "93787/93787 [==============================] - 88s 939us/step - loss: 1.4236 - accuracy: 0.5552\n",
      "Epoch 46/100\n",
      "93787/93787 [==============================] - 86s 920us/step - loss: 1.4212 - accuracy: 0.5563\n",
      "Epoch 47/100\n",
      "93787/93787 [==============================] - 89s 948us/step - loss: 1.4160 - accuracy: 0.5581\n",
      "Epoch 48/100\n",
      "93787/93787 [==============================] - 86s 917us/step - loss: 1.4380 - accuracy: 0.5506\n",
      "Epoch 49/100\n",
      "93787/93787 [==============================] - 77s 822us/step - loss: 1.4178 - accuracy: 0.5558\n",
      "Epoch 50/100\n",
      "93787/93787 [==============================] - 81s 861us/step - loss: 1.4068 - accuracy: 0.5602\n",
      "Epoch 51/100\n",
      "93787/93787 [==============================] - 85s 902us/step - loss: 1.4079 - accuracy: 0.5603\n",
      "Epoch 52/100\n",
      "93787/93787 [==============================] - 87s 931us/step - loss: 1.4060 - accuracy: 0.5592\n",
      "Epoch 53/100\n",
      "93787/93787 [==============================] - 78s 831us/step - loss: 1.4025 - accuracy: 0.5600\n",
      "Epoch 54/100\n",
      "93787/93787 [==============================] - 83s 885us/step - loss: 1.3990 - accuracy: 0.5619\n",
      "Epoch 55/100\n",
      "93787/93787 [==============================] - 85s 905us/step - loss: 1.3961 - accuracy: 0.5623\n",
      "Epoch 56/100\n",
      "93787/93787 [==============================] - 83s 886us/step - loss: 1.3931 - accuracy: 0.5628\n",
      "Epoch 57/100\n",
      "93787/93787 [==============================] - 88s 943us/step - loss: 1.3909 - accuracy: 0.5638\n",
      "Epoch 58/100\n",
      "93787/93787 [==============================] - 95s 1ms/step - loss: 1.3889 - accuracy: 0.5649\n",
      "Epoch 59/100\n",
      "93787/93787 [==============================] - 85s 908us/step - loss: 1.3850 - accuracy: 0.5659\n",
      "Epoch 60/100\n",
      "93787/93787 [==============================] - 88s 942us/step - loss: 1.3831 - accuracy: 0.5664\n",
      "Epoch 61/100\n",
      "93787/93787 [==============================] - 94s 1ms/step - loss: 1.3800 - accuracy: 0.5667\n",
      "Epoch 62/100\n",
      "93787/93787 [==============================] - 118s 1ms/step - loss: 1.3773 - accuracy: 0.5687\n",
      "Epoch 63/100\n",
      "93787/93787 [==============================] - 89s 951us/step - loss: 1.3743 - accuracy: 0.5678\n",
      "Epoch 64/100\n",
      "93787/93787 [==============================] - 78s 828us/step - loss: 1.3722 - accuracy: 0.5694\n",
      "Epoch 65/100\n",
      "93787/93787 [==============================] - 88s 935us/step - loss: 1.3684 - accuracy: 0.5706\n",
      "Epoch 66/100\n",
      "93787/93787 [==============================] - 90s 961us/step - loss: 1.3669 - accuracy: 0.5696\n",
      "Epoch 67/100\n",
      "93787/93787 [==============================] - 89s 944us/step - loss: 1.3638 - accuracy: 0.5717\n",
      "Epoch 68/100\n",
      "93787/93787 [==============================] - 90s 957us/step - loss: 1.3606 - accuracy: 0.5716\n",
      "Epoch 69/100\n",
      "93787/93787 [==============================] - 90s 960us/step - loss: 1.3590 - accuracy: 0.5726\n",
      "Epoch 70/100\n",
      "93787/93787 [==============================] - 89s 949us/step - loss: 1.3563 - accuracy: 0.5728\n",
      "Epoch 71/100\n",
      "93787/93787 [==============================] - 88s 940us/step - loss: 1.3537 - accuracy: 0.5734\n",
      "Epoch 72/100\n",
      "93787/93787 [==============================] - 136s 1ms/step - loss: 1.3514 - accuracy: 0.5742\n",
      "Epoch 73/100\n",
      "93787/93787 [==============================] - 95s 1ms/step - loss: 1.3492 - accuracy: 0.5747\n",
      "Epoch 74/100\n",
      "93787/93787 [==============================] - 97s 1ms/step - loss: 1.3469 - accuracy: 0.5757\n",
      "Epoch 75/100\n",
      "93787/93787 [==============================] - 94s 1ms/step - loss: 1.3441 - accuracy: 0.5779\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93787/93787 [==============================] - 86s 920us/step - loss: 1.3413 - accuracy: 0.5785\n",
      "Epoch 77/100\n",
      "93787/93787 [==============================] - 85s 905us/step - loss: 1.3387 - accuracy: 0.5780\n",
      "Epoch 78/100\n",
      "93787/93787 [==============================] - 85s 910us/step - loss: 1.3373 - accuracy: 0.5785\n",
      "Epoch 79/100\n",
      "93787/93787 [==============================] - 84s 899us/step - loss: 1.3347 - accuracy: 0.5789\n",
      "Epoch 80/100\n",
      "93787/93787 [==============================] - 85s 905us/step - loss: 1.3323 - accuracy: 0.5792\n",
      "Epoch 81/100\n",
      "93787/93787 [==============================] - 85s 911us/step - loss: 1.3306 - accuracy: 0.5790\n",
      "Epoch 82/100\n",
      "93787/93787 [==============================] - 86s 912us/step - loss: 1.3274 - accuracy: 0.5811\n",
      "Epoch 83/100\n",
      "93787/93787 [==============================] - 85s 903us/step - loss: 1.3255 - accuracy: 0.5812\n",
      "Epoch 84/100\n",
      "93787/93787 [==============================] - 85s 907us/step - loss: 1.3229 - accuracy: 0.5819\n",
      "Epoch 85/100\n",
      "93787/93787 [==============================] - 85s 908us/step - loss: 1.3205 - accuracy: 0.5822\n",
      "Epoch 86/100\n",
      "93787/93787 [==============================] - 135s 1ms/step - loss: 1.3187 - accuracy: 0.5833\n",
      "Epoch 87/100\n",
      "93787/93787 [==============================] - 95s 1ms/step - loss: 1.3162 - accuracy: 0.5839\n",
      "Epoch 88/100\n",
      "93787/93787 [==============================] - 95s 1ms/step - loss: 1.3139 - accuracy: 0.5841\n",
      "Epoch 89/100\n",
      "93787/93787 [==============================] - 92s 985us/step - loss: 1.3119 - accuracy: 0.5852\n",
      "Epoch 90/100\n",
      "93787/93787 [==============================] - 92s 983us/step - loss: 1.3107 - accuracy: 0.5858\n",
      "Epoch 91/100\n",
      "93787/93787 [==============================] - 87s 930us/step - loss: 1.3080 - accuracy: 0.5858\n",
      "Epoch 92/100\n",
      "93787/93787 [==============================] - 87s 932us/step - loss: 1.3074 - accuracy: 0.5861\n",
      "Epoch 93/100\n",
      "93787/93787 [==============================] - 130s 1ms/step - loss: 1.3048 - accuracy: 0.5883\n",
      "Epoch 94/100\n",
      "93787/93787 [==============================] - 91s 966us/step - loss: 1.3025 - accuracy: 0.5885\n",
      "Epoch 95/100\n",
      "93787/93787 [==============================] - 93s 988us/step - loss: 1.3000 - accuracy: 0.5897\n",
      "Epoch 96/100\n",
      "93787/93787 [==============================] - 93s 993us/step - loss: 1.2985 - accuracy: 0.5885\n",
      "Epoch 97/100\n",
      "93787/93787 [==============================] - 90s 962us/step - loss: 1.2970 - accuracy: 0.5893\n",
      "Epoch 98/100\n",
      "93787/93787 [==============================] - 88s 937us/step - loss: 1.2960 - accuracy: 0.5890\n",
      "Epoch 99/100\n",
      "93787/93787 [==============================] - 91s 975us/step - loss: 1.2940 - accuracy: 0.5905\n",
      "Epoch 100/100\n",
      "93787/93787 [==============================] - 88s 941us/step - loss: 1.2919 - accuracy: 0.5916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x62b8ba250>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model2.h5')\n",
    "dump(mapping, open('mapping2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from pickle import load\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of characters\n",
    "\tfor _ in range(n_chars):\n",
    "\t\t# encode the characters as integers\n",
    "\t\tencoded = [mapping[char] for char in in_text]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# one hot encode\n",
    "\t\tencoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "\t\t# predict character\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# reverse map integer to character\n",
    "\t\tout_char = ''\n",
    "\t\tfor char, index in mapping.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_char = char\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += char\n",
    "\treturn in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# load the mapping\n",
    "mapping = load(open('mapping.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o know sweet love i always write of you, and in my self thou art conscaied with the heart, which i think of men his stars no remember to thee, and then thou art thee to thee, and then thou art thee, and i which it thy self thou art thee, when i cannot be for my self and thee shall bear the store the world to thee the world that i have seem with a have should love thee the worst that thou dost thou thy self thou art confounding, and thou the charter as thou wilt bountess come, and therefore sees shade, that thou dost thou thy self thou art confounding, and thou the charter as thou wilt bountess come, and therefore sees shade, that tho\n"
     ]
    }
   ],
   "source": [
    "# test start of rhyme\n",
    "print(generate_seq(model, mapping, 40, \"o know sweet love i always write of you, \", 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
