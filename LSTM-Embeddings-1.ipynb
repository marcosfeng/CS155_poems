{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "data = load_doc('data/shakespeare.txt')\n",
    "\n",
    "#Tokenize\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "#Find total number of words for the loop\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "# Transform text into list of numbers...\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    # Create n-grams\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "# add zeros to the beginnings of sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "# Create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "# One-hot encoding\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 300)           1015800   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10, 300)           541200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1693)              170993    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3386)              5735884   \n",
      "=================================================================\n",
      "Total params: 7,624,277\n",
      "Trainable params: 7,624,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "model = Sequential()\n",
    "# Adding word embeddings here with 100 dimensions\n",
    "model.add(Embedding(total_words, 300, input_length=max_sequence_len-1))\n",
    "# Bidirectional LSTM to keep information from both past and future...\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# Final layer, size = number of words in corpus\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15430 samples\n",
      "Epoch 1/100\n",
      "15430/15430 [==============================] - 35s 2ms/sample - loss: 6.8996 - accuracy: 0.0209\n",
      "Epoch 2/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 6.4940 - accuracy: 0.0214\n",
      "Epoch 3/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 6.3848 - accuracy: 0.0255\n",
      "Epoch 4/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 6.2644 - accuracy: 0.0312\n",
      "Epoch 5/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 6.1747 - accuracy: 0.0353\n",
      "Epoch 6/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 6.1002 - accuracy: 0.0392\n",
      "Epoch 7/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 6.0226 - accuracy: 0.0406\n",
      "Epoch 8/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 5.9378 - accuracy: 0.0430\n",
      "Epoch 9/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 5.8403 - accuracy: 0.0495\n",
      "Epoch 10/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 5.7301 - accuracy: 0.0553\n",
      "Epoch 11/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 5.6241 - accuracy: 0.0595\n",
      "Epoch 12/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 5.5169 - accuracy: 0.0676\n",
      "Epoch 13/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 5.4056 - accuracy: 0.0740\n",
      "Epoch 14/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 5.2946 - accuracy: 0.0819\n",
      "Epoch 15/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 5.1930 - accuracy: 0.0894\n",
      "Epoch 16/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 5.0861 - accuracy: 0.0988\n",
      "Epoch 17/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.9851 - accuracy: 0.1046\n",
      "Epoch 18/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.8828 - accuracy: 0.1157\n",
      "Epoch 19/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.7752 - accuracy: 0.1235\n",
      "Epoch 20/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.6733 - accuracy: 0.1355\n",
      "Epoch 21/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.5718 - accuracy: 0.1415\n",
      "Epoch 22/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.4684 - accuracy: 0.1570\n",
      "Epoch 23/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.3615 - accuracy: 0.1665\n",
      "Epoch 24/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.2545 - accuracy: 0.1824\n",
      "Epoch 25/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.1499 - accuracy: 0.1932\n",
      "Epoch 26/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 4.0509 - accuracy: 0.2059\n",
      "Epoch 27/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.9467 - accuracy: 0.2190\n",
      "Epoch 28/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.8397 - accuracy: 0.2384\n",
      "Epoch 29/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.7365 - accuracy: 0.2531\n",
      "Epoch 30/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.6457 - accuracy: 0.2708\n",
      "Epoch 31/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.5439 - accuracy: 0.2866\n",
      "Epoch 32/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.4515 - accuracy: 0.3064\n",
      "Epoch 33/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.3586 - accuracy: 0.3275\n",
      "Epoch 34/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.2649 - accuracy: 0.3462\n",
      "Epoch 35/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.1828 - accuracy: 0.3686\n",
      "Epoch 36/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.1033 - accuracy: 0.3844\n",
      "Epoch 37/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 3.0260 - accuracy: 0.4047\n",
      "Epoch 38/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.9543 - accuracy: 0.4172\n",
      "Epoch 39/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.8779 - accuracy: 0.4396\n",
      "Epoch 40/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.8063 - accuracy: 0.4543\n",
      "Epoch 41/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.7372 - accuracy: 0.4678\n",
      "Epoch 42/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.6660 - accuracy: 0.4854\n",
      "Epoch 43/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.5991 - accuracy: 0.5003\n",
      "Epoch 44/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.5516 - accuracy: 0.5133\n",
      "Epoch 45/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.4832 - accuracy: 0.5255\n",
      "Epoch 46/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.4224 - accuracy: 0.5419\n",
      "Epoch 47/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.3642 - accuracy: 0.5529\n",
      "Epoch 48/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.3095 - accuracy: 0.5653\n",
      "Epoch 49/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.2674 - accuracy: 0.5797\n",
      "Epoch 50/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.2099 - accuracy: 0.5869\n",
      "Epoch 51/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.1676 - accuracy: 0.6010\n",
      "Epoch 52/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.1101 - accuracy: 0.6099\n",
      "Epoch 53/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.0740 - accuracy: 0.6201\n",
      "Epoch 54/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 2.0274 - accuracy: 0.6292\n",
      "Epoch 55/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.9939 - accuracy: 0.6349\n",
      "Epoch 56/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.9431 - accuracy: 0.6472\n",
      "Epoch 57/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.9054 - accuracy: 0.6586\n",
      "Epoch 58/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.8825 - accuracy: 0.6618\n",
      "Epoch 59/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.8506 - accuracy: 0.6649\n",
      "Epoch 60/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.8069 - accuracy: 0.6766\n",
      "Epoch 61/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.7658 - accuracy: 0.6842\n",
      "Epoch 62/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.7468 - accuracy: 0.6854\n",
      "Epoch 63/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.7010 - accuracy: 0.7014\n",
      "Epoch 64/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.6776 - accuracy: 0.7037\n",
      "Epoch 65/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.6507 - accuracy: 0.7083\n",
      "Epoch 66/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.6249 - accuracy: 0.7134\n",
      "Epoch 67/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.6014 - accuracy: 0.7173\n",
      "Epoch 68/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.5688 - accuracy: 0.7312\n",
      "Epoch 69/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.5492 - accuracy: 0.7298\n",
      "Epoch 70/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.5291 - accuracy: 0.7361\n",
      "Epoch 71/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.4960 - accuracy: 0.7414\n",
      "Epoch 72/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.4829 - accuracy: 0.7429\n",
      "Epoch 73/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.4583 - accuracy: 0.7442\n",
      "Epoch 74/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.4475 - accuracy: 0.7494\n",
      "Epoch 75/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.4174 - accuracy: 0.7585\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.4005 - accuracy: 0.7587\n",
      "Epoch 77/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.3935 - accuracy: 0.7574\n",
      "Epoch 78/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.3776 - accuracy: 0.7614\n",
      "Epoch 79/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.3489 - accuracy: 0.7673\n",
      "Epoch 80/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.3402 - accuracy: 0.7681\n",
      "Epoch 81/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.3192 - accuracy: 0.7726\n",
      "Epoch 82/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.3091 - accuracy: 0.7738\n",
      "Epoch 83/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.2908 - accuracy: 0.7768\n",
      "Epoch 84/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.2665 - accuracy: 0.7828\n",
      "Epoch 85/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.2596 - accuracy: 0.7852\n",
      "Epoch 86/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.2456 - accuracy: 0.7846\n",
      "Epoch 87/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.2419 - accuracy: 0.7854\n",
      "Epoch 88/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.2269 - accuracy: 0.7867\n",
      "Epoch 89/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.2200 - accuracy: 0.7878\n",
      "Epoch 90/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1983 - accuracy: 0.7938\n",
      "Epoch 91/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1804 - accuracy: 0.7954\n",
      "Epoch 92/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1800 - accuracy: 0.7968\n",
      "Epoch 93/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1774 - accuracy: 0.7944\n",
      "Epoch 94/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1645 - accuracy: 0.7968\n",
      "Epoch 95/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1576 - accuracy: 0.7970\n",
      "Epoch 96/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1363 - accuracy: 0.8010\n",
      "Epoch 97/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1257 - accuracy: 0.8040\n",
      "Epoch 98/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1227 - accuracy: 0.8025\n",
      "Epoch 99/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.1132 - accuracy: 0.8049\n",
      "Epoch 100/100\n",
      "15430/15430 [==============================] - 32s 2ms/sample - loss: 1.0962 - accuracy: 0.8069\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, label, epochs=100, verbose=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model-embeddings.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o know sweet love i always write of you be slain lie hid be leave thee look it see her summer heaven work thee half thy parts will thine be of good mind foot lies writ hate live some in heaven sun will look so sun hate blind smell more best sweet tongue you\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "seed_text = \"o know sweet love\"\n",
    "next_words = 50\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
